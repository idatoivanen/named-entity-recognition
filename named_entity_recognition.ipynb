{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity recognition\n",
    "\n",
    "- Named entity recognition performed to a Finnish dataset.\n",
    "- FinBERT [(Virtanen et al., 2019)](https://arxiv.org/pdf/1912.07076.pdf) is used as the backbone.\n",
    "- [Huggingface tutorial](https://github.com/huggingface/transformers/blob/main/examples/pytorch/token-classification/run_ner_no_trainer.py) was used for this notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries, and define config class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Fine-tuning a ðŸ¤— Transformers model on token classification tasks (NER, POS, CHUNKS) relying on the accelerate library\n",
    "without using a Trainer.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "from datasets import ClassLabel, load_dataset, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "# from accelerate.logging import get_logger\n",
    "import logging\n",
    "\n",
    "from huggingface_hub import Repository\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    PretrainedConfig,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    ")\n",
    "#from transformers.utils import get_full_repo_name\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "# logger = get_logger(__name__)\n",
    "logger = logging.getLogger(__name__)\n",
    "# require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/token-classification/requirements.txt\")\n",
    "\n",
    "# You should update this to your particular problem to have better documentation of `model_type`\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainConfig():\n",
    "    #A csv or a json file containing the training/validation data\n",
    "    train_file= \"traindata.csv\"\n",
    "    validation_file= \"testdata.csv\"\n",
    "    text_column_name=\"lause2\"\n",
    "    label_column_name= \"ner_tag\"\n",
    "    max_length=250\n",
    "    pad_to_max_length=True #False #True\n",
    "    model_name_or_path= \"\"\n",
    "    per_device_train_batch_size=16\n",
    "    per_device_eval_batch_size=16\n",
    "    learning_rate=5e-5\n",
    "    weight_decay=0.0\n",
    "    num_train_epochs=10\n",
    "    max_train_steps=None #300\n",
    "    #Number of updates steps to accumulate before performing a backward/update pass.\n",
    "    gradient_accumulation_steps=1\n",
    "    lr_scheduler_type=\"linear\"\n",
    "    num_warmup_steps=2\n",
    "    output_dir= \"tc_output\"\n",
    "    seed=22020\n",
    "    model_type= \"TurkuNLP/bert-base-finnish-cased-v1\"\n",
    "    config_name= \"TurkuNLP/bert-base-finnish-cased-v1\"\n",
    "    tokenizer_name= \"TurkuNLP/bert-base-finnish-cased-v1\"\n",
    "    #Setting labels of all special tokens to -100 and thus PyTorch will ignore them.\n",
    "    label_all_tokens=True\n",
    "    #Indication whether entity level metrics are to be returned\n",
    "    return_entity_level_metrics=True\n",
    "    task_name=\"ner\" #[\"ner\", \"pos\", \"chunk\"]\n",
    "    debug=False #Activate debug mode and run training only with a subset of data\n",
    "    push_to_hub=False #Whether or not to push the model to the Hub.\n",
    "    hub_model_id=None #\"\"\n",
    "    hub_token=\"\" #The token to use to push to the Model Hub.\n",
    "    #Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch\n",
    "    checkpointing_steps=\"\"\n",
    "    resume_from_checkpoint=\"\"\n",
    "    #Whether to load in all available experiment trackers from the environment and use them for logging\n",
    "    with_tracking=True #False\n",
    "    #Whether or not to enable to load a pretrained model whose head dimensions are different.\n",
    "    ignore_mismatched_sizes=False\n",
    "    dataset_name=None\n",
    "\n",
    "args = TrainConfig() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up accelerator, model, datasets etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
    "# If we're using tracking, we also need to initialize it here and it will pick up all supported trackers in the environment\n",
    "# accelerator = Accelerator(logging_dir=args.output_dir) if args.with_tracking else Accelerator()\n",
    "accelerator = Accelerator(log_with=\"all\", logging_dir=args.output_dir) if args.with_tracking else Accelerator()\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state)\n",
    "# logger.info(accelerator.state, main_process_only=False)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# If passed along, set the training seed now.\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "# Handle the repository creation\n",
    "if accelerator.is_main_process:\n",
    "    if args.push_to_hub:\n",
    "        if args.hub_model_id is None:\n",
    "            repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\n",
    "        else:\n",
    "            repo_name = args.hub_model_id\n",
    "        repo = Repository(args.output_dir, clone_from=repo_name)\n",
    "\n",
    "        with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n",
    "            if \"step_*\" not in gitignore:\n",
    "                gitignore.write(\"step_*\\n\")\n",
    "            if \"epoch_*\" not in gitignore:\n",
    "                gitignore.write(\"epoch_*\\n\")\n",
    "    elif args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "accelerator.wait_for_everyone()\n",
    "\n",
    "# Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n",
    "# or just provide the name of one of the public datasets for token classification task available on the hub at https://huggingface.co/datasets/\n",
    "# (the dataset will be downloaded automatically from the datasets Hub).\n",
    "#\n",
    "# For CSV/JSON files, this script will use the column called 'tokens' or the first column if no column called\n",
    "# 'tokens' is found. You can easily tweak this behavior (see below).\n",
    "#\n",
    "# In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "# download the dataset.\n",
    "if args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n",
    "else:\n",
    "    data_files = {}\n",
    "    if args.train_file is not None:\n",
    "        data_files[\"train\"] = args.train_file\n",
    "    if args.validation_file is not None:\n",
    "        data_files[\"validation\"] = args.validation_file\n",
    "    extension = args.train_file.split(\".\")[-1]\n",
    "    raw_datasets = load_dataset(extension, data_files=data_files)\n",
    "# Trim a number of training examples\n",
    "if args.debug:\n",
    "    for split in raw_datasets.keys():\n",
    "        raw_datasets[split] = raw_datasets[split].select(range(100))\n",
    "# See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
    "# https://huggingface.co/docs/datasets/loading_datasets.html.\n",
    "\n",
    "if raw_datasets[\"train\"] is not None:\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "    features = raw_datasets[\"train\"].features\n",
    "else:\n",
    "    column_names = raw_datasets[\"validation\"].column_names\n",
    "    features = raw_datasets[\"validation\"].features\n",
    "\n",
    "if args.text_column_name is not None:\n",
    "    text_column_name = args.text_column_name\n",
    "elif \"tokens\" in column_names:\n",
    "    text_column_name = \"tokens\"\n",
    "else:\n",
    "    text_column_name = column_names[0]\n",
    "\n",
    "if args.label_column_name is not None:\n",
    "    label_column_name = args.label_column_name\n",
    "elif f\"{args.task_name}_tags\" in column_names:\n",
    "    label_column_name = f\"{args.task_name}_tags\"\n",
    "else:\n",
    "    label_column_name = column_names[1]\n",
    "\n",
    "# In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the\n",
    "# unique labels.\n",
    "def get_label_list(labels):\n",
    "    unique_labels = set()\n",
    "    for label in labels:\n",
    "        unique_labels = unique_labels | set(label)\n",
    "    label_list = list(unique_labels)\n",
    "    label_list.sort()\n",
    "    return label_list\n",
    "\n",
    "\n",
    "#the following code omitted \n",
    "\n",
    "# # If the labels are of type ClassLabel, they are already integers and we have the map stored somewhere.\n",
    "# # Otherwise, we have to get the list of labels manually.\n",
    "# labels_are_int = isinstance(features[label_column_name].feature, ClassLabel)\n",
    "# if labels_are_int:\n",
    "#     label_list = features[label_column_name].feature.names\n",
    "#     label_to_id = {i: i for i in range(len(label_list))}\n",
    "# else:\n",
    "#     label_list = get_label_list(raw_datasets[\"train\"][label_column_name])\n",
    "#     label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "\n",
    "# num_labels = len(label_list)\n",
    "\n",
    "#instead we go by:\n",
    "label_list = ['O','s_ang','s_ant', 's_d', 's_f', 's_j', 's_sa', 's_su', 's_t'] #TODO change according to your usecase\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "num_labels = len(label_list)\n",
    "print(label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "if args.config_name:\n",
    "    config = AutoConfig.from_pretrained(args.config_name, num_labels=num_labels)\n",
    "elif args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(args.model_name_or_path, num_labels=num_labels)\n",
    "else:\n",
    "    config = CONFIG_MAPPING[args.model_type]()\n",
    "    logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "\n",
    "tokenizer_name_or_path = args.tokenizer_name if args.tokenizer_name else args.model_name_or_path\n",
    "if not tokenizer_name_or_path:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )\n",
    "\n",
    "if config.model_type in {\"gpt2\", \"roberta\"}:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path, use_fast=True, add_prefix_space=True)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path, use_fast=True)\n",
    "\n",
    "if args.model_name_or_path:\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "        config=config,\n",
    "        ignore_mismatched_sizes=args.ignore_mismatched_sizes,\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Training new model from scratch\")\n",
    "    model = AutoModelForTokenClassification.from_config(config)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# labels_are_int = isinstance(features[label_column_name].feature, ClassLabel)\n",
    "labels_are_int = False\n",
    "\n",
    "# Model has labels -> use them.\n",
    "if model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id:\n",
    "    if list(sorted(model.config.label2id.keys())) == list(sorted(label_list)):\n",
    "        # Reorganize `label_list` to match the ordering of the model.\n",
    "        if labels_are_int:\n",
    "            label_to_id = {i: int(model.config.label2id[l]) for i, l in enumerate(label_list)}\n",
    "            label_list = [model.config.id2label[i] for i in range(num_labels)]\n",
    "        else:\n",
    "            label_list = [model.config.id2label[i] for i in range(num_labels)]\n",
    "            label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "    else:\n",
    "        logger.warning(\n",
    "            \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n",
    "            f\"model labels: {list(sorted(model.config.label2id.keys()))}, dataset labels:\"\n",
    "            f\" {list(sorted(label_list))}.\\nIgnoring the model labels as a result.\",\n",
    "        )\n",
    "\n",
    "# Set the correspondences label/ID inside the model config\n",
    "model.config.label2id = {l: i for i, l in enumerate(label_list)}\n",
    "model.config.id2label = {i: l for i, l in enumerate(label_list)}\n",
    "\n",
    "# Map that sends B-Xxx label to its I-Xxx counterpart\n",
    "b_to_i_label = []\n",
    "for idx, label in enumerate(label_list):\n",
    "    if label.startswith(\"B-\") and label.replace(\"B-\", \"I-\") in label_list:\n",
    "        b_to_i_label.append(label_list.index(label.replace(\"B-\", \"I-\")))\n",
    "    else:\n",
    "        b_to_i_label.append(idx)\n",
    "print(b_to_i_label)\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing the datasets.\n",
    "\n",
    "padding = \"max_length\" if args.pad_to_max_length else False\n",
    "\n",
    "#change the format of each row in examples[text_column_name]\n",
    "def change_format(examples):\n",
    "    tt=[]\n",
    "    for i in examples:\n",
    "        i=i.strip()\n",
    "        tt.append(i.split(' '))\n",
    "    return tt\n",
    "\n",
    "# Tokenize all texts and align the labels with them.\n",
    "def tokenize_and_align_labels(examples):\n",
    "    #change the format of each row in examples[text_column_name]\n",
    "    examples[text_column_name]=change_format(examples[text_column_name])\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[text_column_name],\n",
    "        max_length=args.max_length,\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[label_column_name]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        label=label.replace(\"'\",\"\")\n",
    "        label=label.replace(\" \",\"\")\n",
    "        label=label.strip(\"'[ ]\")\n",
    "        label=label.split(',')\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_to_id[label[word_idx]]) #todo tarkista label_to_id\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                if args.label_all_tokens:\n",
    "                    label_ids.append(b_to_i_label[label_to_id[label[word_idx]]])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    processed_raw_datasets = raw_datasets.map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        remove_columns=raw_datasets[\"train\"].column_names,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "train_dataset = processed_raw_datasets[\"train\"]\n",
    "eval_dataset = processed_raw_datasets[\"validation\"]\n",
    "\n",
    "# Log a few random samples from the training set:\n",
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "# DataLoaders creation:\n",
    "if args.pad_to_max_length:\n",
    "    # If padding was already done ot max length, we use the default data collator that will just convert everything\n",
    "    # to tensors.\n",
    "    data_collator = default_data_collator\n",
    "else:\n",
    "    # Otherwise, `DataCollatorForTokenClassification` will apply dynamic padding for us (by padding to the maximum length of\n",
    "    # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n",
    "    # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
    "    data_collator = DataCollatorForTokenClassification(\n",
    "        tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None)\n",
    "    )\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n",
    ")\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "\n",
    "# Use the device given by the `accelerator` object.\n",
    "device = accelerator.device\n",
    "model.to(device)\n",
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "else:\n",
    "    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.num_warmup_steps,\n",
    "    num_training_steps=args.max_train_steps,\n",
    ")\n",
    "\n",
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "# Figure out how many steps we should save the Accelerator states\n",
    "if hasattr(args.checkpointing_steps, \"isdigit\"):\n",
    "    checkpointing_steps = args.checkpointing_steps\n",
    "    if args.checkpointing_steps.isdigit():\n",
    "        checkpointing_steps = int(args.checkpointing_steps)\n",
    "else:\n",
    "    checkpointing_steps = None\n",
    "\n",
    "# We need to initialize the trackers we use, and also store our configuration\n",
    "if args.with_tracking:\n",
    "    experiment_config = vars(args)\n",
    "    # TensorBoard cannot log Enums, need the raw value\n",
    "    #experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n",
    "    experiment_config[\"lr_scheduler_type\"] =args.lr_scheduler_type\n",
    "    accelerator.init_trackers(\"ner_no_trainer\", experiment_config)\n",
    "\n",
    "# Metrics\n",
    "metric = load_metric(\"seqeval\")\n",
    "experiment_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(predictions, references):\n",
    "    # Transform predictions and references tensos to numpy arrays\n",
    "    if device.type == \"cpu\":\n",
    "        y_pred = predictions.detach().clone().numpy()\n",
    "        y_true = references.detach().clone().numpy()\n",
    "    else:\n",
    "        y_pred = predictions.detach().cpu().clone().numpy()\n",
    "        y_true = references.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(pred, gold_label) if l != -100]\n",
    "        for pred, gold_label in zip(y_pred, y_true)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(pred, gold_label) if l != -100]\n",
    "        for pred, gold_label in zip(y_pred, y_true)\n",
    "    ]\n",
    "    return true_predictions, true_labels\n",
    "\n",
    "def compute_metrics():\n",
    "    results = metric.compute()\n",
    "    if args.return_entity_level_metrics:\n",
    "        # Unpack nested dictionaries\n",
    "        final_results = {}\n",
    "        for key, value in results.items():\n",
    "            if isinstance(value, dict):\n",
    "                for n, v in value.items():\n",
    "                    final_results[f\"{key}_{n}\"] = v\n",
    "            else:\n",
    "                final_results[key] = value\n",
    "        return final_results\n",
    "    else:\n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train!\n",
    "total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "# Potentially load in the weights and states from a previous save\n",
    "if args.resume_from_checkpoint:\n",
    "    if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n",
    "        accelerator.print(f\"Resumed from checkpoint: {args.resume_from_checkpoint}\")\n",
    "        accelerator.load_state(args.resume_from_checkpoint)\n",
    "        path = os.path.basename(args.resume_from_checkpoint)\n",
    "    else:\n",
    "        # Get the most recent checkpoint\n",
    "        dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n",
    "        dirs.sort(key=os.path.getctime)\n",
    "        path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n",
    "    # Extract `epoch_{i}` or `step_{i}`\n",
    "    training_difference = os.path.splitext(path)[0]\n",
    "\n",
    "    if \"epoch\" in training_difference:\n",
    "        starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n",
    "        resume_step = None\n",
    "    else:\n",
    "        resume_step = int(training_difference.replace(\"step_\", \"\"))\n",
    "        starting_epoch = resume_step // len(train_dataloader)\n",
    "        resume_step -= starting_epoch * len(train_dataloader)\n",
    "\n",
    "for epoch in range(starting_epoch, args.num_train_epochs):\n",
    "    model.train()\n",
    "    if args.with_tracking:\n",
    "        total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # We need to skip steps until we reach the resumed step\n",
    "        if args.resume_from_checkpoint and epoch == starting_epoch:\n",
    "            if resume_step is not None and step < resume_step:\n",
    "                completed_steps += 1\n",
    "                continue\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        # We keep track of the loss at each epoch\n",
    "        if args.with_tracking:\n",
    "            total_loss += loss.detach().float()\n",
    "        loss = loss / args.gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "\n",
    "        if isinstance(checkpointing_steps, int):\n",
    "            if completed_steps % checkpointing_steps == 0:\n",
    "                output_dir = f\"step_{completed_steps }\"\n",
    "                if args.output_dir is not None:\n",
    "                    output_dir = os.path.join(args.output_dir, output_dir)\n",
    "                accelerator.save_state(output_dir)\n",
    "\n",
    "        if completed_steps >= args.max_train_steps:\n",
    "            break\n",
    "\n",
    "    logger.info(f\"  Loss = {loss}\")\n",
    "    model.eval()\n",
    "    samples_seen = 0\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "        if not args.pad_to_max_length:  # necessary to pad predictions and labels for being gathered\n",
    "            predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "            labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "        predictions_gathered, labels_gathered = accelerator.gather((predictions, labels))\n",
    "        # If we are in a multiprocess environment, the last batch has duplicates\n",
    "        if accelerator.num_processes > 1:\n",
    "            if step == len(eval_dataloader) - 1:\n",
    "                predictions_gathered = predictions_gathered[: len(eval_dataloader.dataset) - samples_seen]\n",
    "                labels_gathered = labels_gathered[: len(eval_dataloader.dataset) - samples_seen]\n",
    "            else:\n",
    "                samples_seen += labels_gathered.shape[0]\n",
    "        preds, refs = get_labels(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(\n",
    "            predictions=preds,\n",
    "            references=refs,\n",
    "        )  # predictions and preferences are expected to be a nested list of labels, not label_ids\n",
    "\n",
    "    eval_metric = compute_metrics()\n",
    "    accelerator.print(f\"epoch {epoch}:\", eval_metric)\n",
    "    if args.with_tracking:\n",
    "        accelerator.log(\n",
    "            {\"seqeval\": eval_metric, \"train_loss\": total_loss, \"epoch\": epoch, \"step\": completed_steps},\n",
    "        )\n",
    "\n",
    "    if args.push_to_hub and epoch < args.num_train_epochs - 1:\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(\n",
    "            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "        )\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(args.output_dir)\n",
    "            repo.push_to_hub(\n",
    "                commit_message=f\"Training in progress epoch {epoch}\", blocking=False, auto_lfs_prune=True\n",
    "            )\n",
    "\n",
    "    if args.checkpointing_steps == \"epoch\":\n",
    "        output_dir = f\"epoch_{epoch}\"\n",
    "        if args.output_dir is not None:\n",
    "            output_dir = os.path.join(args.output_dir, output_dir)\n",
    "        accelerator.save_state(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args.output_dir is not None:\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(\n",
    "        args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "        if args.push_to_hub:\n",
    "            repo.push_to_hub(commit_message=\"End of training\", auto_lfs_prune=True)\n",
    "\n",
    "eval_metric\n",
    "#     with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n",
    "#         json.dump({\"eval_accuracy\": eval_metric[\"accuracy\"], \"train_loss\": float(loss.cpu().detach().numpy())}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestConfig():\n",
    "    #A csv or a json file containing the training/validation data\n",
    "    train_file= \"traindata.csv\"\n",
    "    validation_file= \"validdata.csv\"\n",
    "    text_column_name=\"lause2\"\n",
    "    label_column_name= \"ner_tag\"\n",
    "    max_length=250\n",
    "    pad_to_max_length=True #False #True\n",
    "    model_name_or_path= \"\"\n",
    "    per_device_train_batch_size=16\n",
    "    per_device_eval_batch_size=16\n",
    "    learning_rate=5e-5\n",
    "    weight_decay=0.0\n",
    "    num_train_epochs=10\n",
    "    max_train_steps=None #300\n",
    "    #Number of updates steps to accumulate before performing a backward/update pass.\n",
    "    gradient_accumulation_steps=1\n",
    "    lr_scheduler_type=\"linear\"\n",
    "    num_warmup_steps=2\n",
    "    output_dir= \"tc_output\"\n",
    "    seed=22020\n",
    "    model_type= \"TurkuNLP/bert-base-finnish-cased-v1\"\n",
    "    config_name= \"TurkuNLP/bert-base-finnish-cased-v1\"\n",
    "    tokenizer_name= \"TurkuNLP/bert-base-finnish-cased-v1\"\n",
    "    #Setting labels of all special tokens to -100 and thus PyTorch will ignore them.\n",
    "    label_all_tokens=True\n",
    "    #Indication whether entity level metrics are to be returner\n",
    "    return_entity_level_metrics=True \n",
    "    task_name=\"ner\" #[\"ner\", \"pos\", \"chunk\"]\n",
    "    debug=False #Activate debug mode and run training only with a subset of data\n",
    "    push_to_hub=False #Whether or not to push the model to the Hub.\n",
    "    hub_model_id=None #\"\"\n",
    "    hub_token=\"\" #The token to use to push to the Model Hub.\n",
    "    #Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch\n",
    "    checkpointing_steps=\"\"\n",
    "    resume_from_checkpoint=\"\"\n",
    "    #Whether to load in all available experiment trackers from the environment and use them for logging\n",
    "    with_tracking=True\n",
    "    #Whether or not to enable to load a pretrained model whose head dimensions are different.\n",
    "    ignore_mismatched_sizes=False\n",
    "    dataset_name=None\n",
    "\n",
    "args = TestConfig() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
    "# If we're using tracking, we also need to initialize it here and it will pick up all supported trackers in the environment\n",
    "# accelerator = Accelerator(logging_dir=args.output_dir) if args.with_tracking else Accelerator()\n",
    "accelerator = Accelerator(log_with=\"all\", logging_dir=args.output_dir) if args.with_tracking else Accelerator()\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state)\n",
    "# logger.info(accelerator.state, main_process_only=False)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# If passed along, set the training seed now.\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "# Handle the repository creation\n",
    "if accelerator.is_main_process:\n",
    "    if args.push_to_hub:\n",
    "        if args.hub_model_id is None:\n",
    "            repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\n",
    "        else:\n",
    "            repo_name = args.hub_model_id\n",
    "        repo = Repository(args.output_dir, clone_from=repo_name)\n",
    "\n",
    "        with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n",
    "            if \"step_*\" not in gitignore:\n",
    "                gitignore.write(\"step_*\\n\")\n",
    "            if \"epoch_*\" not in gitignore:\n",
    "                gitignore.write(\"epoch_*\\n\")\n",
    "    elif args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "accelerator.wait_for_everyone()\n",
    "\n",
    "# Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n",
    "# or just provide the name of one of the public datasets for token classification task available on the hub at https://huggingface.co/datasets/\n",
    "# (the dataset will be downloaded automatically from the datasets Hub).\n",
    "#\n",
    "# For CSV/JSON files, this script will use the column called 'tokens' or the first column if no column called\n",
    "# 'tokens' is found. You can easily tweak this behavior (see below).\n",
    "#\n",
    "# In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "# download the dataset.\n",
    "if args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n",
    "else:\n",
    "    data_files = {}\n",
    "    if args.train_file is not None:\n",
    "        data_files[\"train\"] = args.train_file\n",
    "    if args.validation_file is not None:\n",
    "        data_files[\"validation\"] = args.validation_file\n",
    "    extension = args.train_file.split(\".\")[-1]\n",
    "    raw_datasets = load_dataset(extension, data_files=data_files)\n",
    "# Trim a number of training examples\n",
    "if args.debug:\n",
    "    for split in raw_datasets.keys():\n",
    "        raw_datasets[split] = raw_datasets[split].select(range(100))\n",
    "# See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
    "# https://huggingface.co/docs/datasets/loading_datasets.html.\n",
    "\n",
    "if raw_datasets[\"train\"] is not None:\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "    features = raw_datasets[\"train\"].features\n",
    "else:\n",
    "    column_names = raw_datasets[\"validation\"].column_names\n",
    "    features = raw_datasets[\"validation\"].features\n",
    "\n",
    "if args.text_column_name is not None:\n",
    "    text_column_name = args.text_column_name\n",
    "elif \"tokens\" in column_names:\n",
    "    text_column_name = \"tokens\"\n",
    "else:\n",
    "    text_column_name = column_names[0]\n",
    "\n",
    "if args.label_column_name is not None:\n",
    "    label_column_name = args.label_column_name\n",
    "elif f\"{args.task_name}_tags\" in column_names:\n",
    "    label_column_name = f\"{args.task_name}_tags\"\n",
    "else:\n",
    "    label_column_name = column_names[1]\n",
    "\n",
    "# In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the\n",
    "# unique labels.\n",
    "def get_label_list(labels):\n",
    "    unique_labels = set()\n",
    "    for label in labels:\n",
    "        unique_labels = unique_labels | set(label)\n",
    "    label_list = list(unique_labels)\n",
    "    label_list.sort()\n",
    "    return label_list\n",
    "\n",
    "#the following code omitted \n",
    "\n",
    "# # If the labels are of type ClassLabel, they are already integers and we have the map stored somewhere.\n",
    "# # Otherwise, we have to get the list of labels manually.\n",
    "# labels_are_int = isinstance(features[label_column_name].feature, ClassLabel)\n",
    "# if labels_are_int:\n",
    "#     label_list = features[label_column_name].feature.names\n",
    "#     label_to_id = {i: i for i in range(len(label_list))}\n",
    "# else:\n",
    "#     label_list = get_label_list(raw_datasets[\"train\"][label_column_name])\n",
    "#     label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "\n",
    "# num_labels = len(label_list)\n",
    "\n",
    "#instead we go by:\n",
    "label_list = ['O','s_ang','s_ant', 's_d', 's_f', 's_j', 's_sa', 's_su', 's_t']\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "num_labels = len(label_list)\n",
    "print(label_to_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_are_int = isinstance(features[label_column_name].feature, ClassLabel)\n",
    "labels_are_int = False\n",
    "\n",
    "# Model has labels -> use them.\n",
    "if model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id:\n",
    "    if list(sorted(model.config.label2id.keys())) == list(sorted(label_list)):\n",
    "        # Reorganize `label_list` to match the ordering of the model.\n",
    "        if labels_are_int:\n",
    "            label_to_id = {i: int(model.config.label2id[l]) for i, l in enumerate(label_list)}\n",
    "            label_list = [model.config.id2label[i] for i in range(num_labels)]\n",
    "        else:\n",
    "            label_list = [model.config.id2label[i] for i in range(num_labels)]\n",
    "            label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "    else:\n",
    "        logger.warning(\n",
    "            \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n",
    "            f\"model labels: {list(sorted(model.config.label2id.keys()))}, dataset labels:\"\n",
    "            f\" {list(sorted(label_list))}.\\nIgnoring the model labels as a result.\",\n",
    "        )\n",
    "\n",
    "# Set the correspondences label/ID inside the model config\n",
    "model.config.label2id = {l: i for i, l in enumerate(label_list)}\n",
    "model.config.id2label = {i: l for i, l in enumerate(label_list)}\n",
    "\n",
    "# Map that sends B-Xxx label to its I-Xxx counterpart\n",
    "b_to_i_label = []\n",
    "for idx, label in enumerate(label_list):\n",
    "    if label.startswith(\"B-\") and label.replace(\"B-\", \"I-\") in label_list:\n",
    "        b_to_i_label.append(label_list.index(label.replace(\"B-\", \"I-\")))\n",
    "    else:\n",
    "        b_to_i_label.append(idx)\n",
    "print(b_to_i_label)\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing the datasets.\n",
    "\n",
    "padding = \"max_length\" if args.pad_to_max_length else False\n",
    "\n",
    "#change the format of each row in examples[text_column_name]\n",
    "def change_format(examples):\n",
    "    tt=[]\n",
    "    for i in examples:\n",
    "        i=i.strip()\n",
    "        tt.append(i.split(' '))\n",
    "    return tt\n",
    "\n",
    "# Tokenize all texts and align the labels with them.\n",
    "def tokenize_and_align_labels(examples):\n",
    "    examples[text_column_name]=change_format(examples[text_column_name])\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[text_column_name],\n",
    "        max_length=args.max_length,\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[label_column_name]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        label=label.replace(\"'\",\"\")\n",
    "        label=label.replace(\" \",\"\")\n",
    "        label=label.strip(\"'[ ]\")\n",
    "        label=label.split(',')\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_to_id[label[word_idx]]) #todo tarkista label_to_id\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                if args.label_all_tokens:\n",
    "                    label_ids.append(b_to_i_label[label_to_id[label[word_idx]]])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    processed_raw_datasets = raw_datasets.map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        remove_columns=raw_datasets[\"train\"].column_names,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "train_dataset = processed_raw_datasets[\"train\"]\n",
    "eval_dataset = processed_raw_datasets[\"validation\"]\n",
    "\n",
    "# Log a few random samples from the training set:\n",
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "# DataLoaders creation:\n",
    "if args.pad_to_max_length:\n",
    "    # If padding was already done ot max length, we use the default data collator that will just convert everything\n",
    "    # to tensors.\n",
    "    data_collator = default_data_collator\n",
    "else:\n",
    "    # Otherwise, `DataCollatorForTokenClassification` will apply dynamic padding for us (by padding to the maximum length of\n",
    "    # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n",
    "    # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
    "    data_collator = DataCollatorForTokenClassification(\n",
    "        tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None)\n",
    "    )\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n",
    ")\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = accelerator.device\n",
    "model.to(device)\n",
    "\n",
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "metric = load_metric(\"seqeval\")\n",
    "experiment_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "samples_seen = 0\n",
    "for step, batch in enumerate(eval_dataloader):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    predictions = outputs.logits.argmax(dim=-1)\n",
    "    labels = batch[\"labels\"]\n",
    "    if not args.pad_to_max_length:  # necessary to pad predictions and labels for being gathered\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "    predictions_gathered, labels_gathered = accelerator.gather((predictions, labels))\n",
    "    # If we are in a multiprocess environment, the last batch has duplicates\n",
    "    if accelerator.num_processes > 1:\n",
    "        if step == len(eval_dataloader) - 1:\n",
    "            predictions_gathered = predictions_gathered[: len(eval_dataloader.dataset) - samples_seen]\n",
    "            labels_gathered = labels_gathered[: len(eval_dataloader.dataset) - samples_seen]\n",
    "        else:\n",
    "            samples_seen += labels_gathered.shape[0]\n",
    "    preds, refs = get_labels(predictions_gathered, labels_gathered)\n",
    "    metric.add_batch(\n",
    "        predictions=preds,\n",
    "        references=refs,\n",
    "    )  # predictions and preferences are expected to be a nested list of labels, not label_ids\n",
    "\n",
    "eval_metric = compute_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_metric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
