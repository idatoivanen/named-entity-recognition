{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data\n",
    "\n",
    "- Preprocessing data that is used for named entity recognition task. The data has been lemmatized beforehand.\n",
    "- Preprocessing includes \n",
    "    1. collecting wordlists from Emolex and modifying them \\\n",
    "        a) with simple preprocessing function (process_wordlist), \\\n",
    "        b) by hand (removing irrelevant words (e.g. english words), adding words that would support the lemmatizer's properties (e.g. for word \"annoyance\" there are multiple conjugations (ärtyneisyys, ärtymys))),\n",
    "    2. NER tagging data, and\n",
    "    3. dividing data into different subsets.\n",
    "- Wordlists are based on Emolex (the following research): \\\n",
    "    Mohammad, S. M., & Turney, P. D. (2013). Crowdsourcing a word–emotion association lexicon. Computational intelligence, 29(3), 436-465. [x](https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)\n",
    "- This notebook utilized the older version of Finnish Emolex. \n",
    "- All in all, there are nine classes (nine different NER tags):\n",
    "    1. O = other,\n",
    "    2. s_ang = sentiment_angry,\n",
    "    3. s_ant = anticipation, \n",
    "    4. s_d = disgust, \n",
    "    5. s_f = fear, \n",
    "    6. s_j = joy, \n",
    "    7. s_sa = sad, \n",
    "    8. s_su = surprise, \n",
    "    9. s_t = trust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load dataset that has been lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are six columns (id, lause (sentence), len, lause2 (sentence2), lemmat (lemmatized), vuosi (year))\n",
    "df = pd.read_csv('./nerdata.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are NAN values in year column (\"vuosi\"), so we fill them up correctly.\n",
    "df[df.vuosi.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vuosi']=df['vuosi'].fillna(2019.0)\n",
    "df.vuosi.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"vuosi\"] = df[\"vuosi\"].astype(np.int64)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('nerdata.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modifying wordlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wordlist(f):\n",
    "    df=pd.read_csv(f)\n",
    "    print(\"orig length \",len(df))\n",
    "    print('duplicates: ',df.word2.duplicated().sum())\n",
    "    df=df[df.word2!=\"NO TRANSLATION\"]\n",
    "    df=df.drop_duplicates(subset=['word2'])\n",
    "    df=df[(df['emotion-intensity-score']>0.5)]\n",
    "    df=df.reset_index(drop=True)\n",
    "    print(\"length after changes \",len(df))\n",
    "    return df.word2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angy2=process_wordlist('df_anger.csv')\n",
    "angy2.sort()\n",
    "angy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anti=process_wordlist('df_anticipation.csv')\n",
    "anti.sort()\n",
    "anti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "disg=process_wordlist('df_disgust.csv')\n",
    "disg.sort()\n",
    "disg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fear=process_wordlist('df_fear.csv')\n",
    "fear.sort()\n",
    "fear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joy=process_wordlist('df_joy.csv')\n",
    "joy.sort()\n",
    "joy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sad=process_wordlist('df_sadness.csv')\n",
    "sad.sort()\n",
    "sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "surp=process_wordlist('df_surprise.csv')\n",
    "surp.sort()\n",
    "surp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trust=process_wordlist('df_trust.csv')\n",
    "trust.sort()\n",
    "trust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add NER tags with the help of wordlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note, that there has been manual changes between word list versions (e.g., df_anti and df_anti2).\n",
    "\n",
    "anti = pd.read_csv('df_anti2.csv')\n",
    "anti=anti.anticipation.values\n",
    "joy = pd.read_csv('df_joy2.csv')\n",
    "joy=joy.joy.values\n",
    "trust = pd.read_csv('df_trust2.csv')\n",
    "trust=trust.trust.values\n",
    "angy = pd.read_csv('df_anger2.csv')\n",
    "angy=angy.anger.values\n",
    "surp = pd.read_csv('df_surprise2.csv')\n",
    "surp=surp.surprise.values\n",
    "sad = pd.read_csv('df_sad2.csv')\n",
    "sad=sad.sad.values\n",
    "fear = pd.read_csv('df_fear2.csv')\n",
    "fear=fear.fear.values\n",
    "disg = pd.read_csv('df_disgust2.csv')\n",
    "disg=disg.disgust.values\n",
    "\n",
    "print(len(anti))\n",
    "print(len(joy))\n",
    "print(len(trust))\n",
    "print(len(angy))\n",
    "print(len(surp))\n",
    "print(len(sad))\n",
    "print(len(fear))\n",
    "print(len(disg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"nerdata.csv\")\n",
    "print(type(df.lemmat[0]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column \"lemmat\" type changed from string into array\n",
    "from ast import literal_eval    \n",
    "df[\"lemmat\"] = df[\"lemmat\"].apply(lambda x: literal_eval(x))\n",
    "print(type(df.lemmat[0]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to add empty ner_tag arrays\n",
    "def add_nertag_arrays(sent_array):\n",
    "    return [\"O\"]*len(sent_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ner tag initialization (with \"O\") for every data row\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "df[\"ner_tag\"] = df[\"lemmat\"].progress_apply(add_nertag_arrays)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding ner tags (comparing words from one sentiment word list with the lemmatized words in our dataset)\n",
    "def add_ner_tags_for_group(x,sent_list,sent_name,df):\n",
    "    for j in range(len(df.lemmat[x])):\n",
    "        for i in range(len(sent_list)):\n",
    "            if df.lemmat[x][j] == sent_list[i]:\n",
    "                df.ner_tag[x][j]=sent_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment lists\n",
    "sent_list_header=[angy, anti, disg, fear, joy, sad, surp, trust]\n",
    "sent_name_header=['s_ang','s_ant', 's_d', 's_f', 's_j', 's_sa', 's_su', 's_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#going through all words in sentiment lists and comparing them to the lemmatized word\n",
    "for i in tqdm(range(len(sent_list_header))):\n",
    "    [add_ner_tags_for_group(x,sent_list_header[i],sent_name_header[i],df) for x in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('ner_data_tagged.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Divide data into train, test and valid sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using kfolds or stratified kfolds to create folds for the data \n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "\n",
    "def create_strat_kfolds(data, num_splits, random_seed):\n",
    "    data[\"kfold\"] = -1\n",
    "    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=random_seed)\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=data.sent,y=data.label)):\n",
    "        data.loc[v_, 'kfold'] = f\n",
    "    return data\n",
    "\n",
    "def create_kfolds(data, num_splits, random_seed):\n",
    "    data[\"kfold\"] = -1\n",
    "    kf = model_selection.KFold(n_splits=num_splits, shuffle=True, random_state=random_seed)\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=data)):\n",
    "        data.loc[v_, 'kfold'] = f\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first validation dataset extracted with kfolds/stratified kfolds\n",
    "df = create_kfolds(df, num_splits=10, random_seed=52467)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid=df[df.kfold==0]\n",
    "traintest=df[df.kfold!=0]\n",
    "valid=valid.reset_index(drop=True)\n",
    "traintest=traintest.reset_index(drop=True)\n",
    "traintest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintest = create_kfolds(traintest, num_splits=10, random_seed=52467)\n",
    "traintest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset=traintest[(traintest.kfold!=2)&(traintest.kfold!=6)]\n",
    "testset=traintest[(traintest.kfold==2)|(traintest.kfold==6)]\n",
    "trainset=trainset.reset_index(drop=True)\n",
    "testset=testset.reset_index(drop=True)\n",
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.to_csv('traindata.csv',index=False)\n",
    "testset.to_csv('testdata.csv',index=False)\n",
    "valid.to_csv('validdata.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
